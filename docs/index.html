<!DOCTYPE html>
<html>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<meta property='og:title' content='VLDBench: Vision Language Models Disinformation Detection Benchmark' />
<meta property='og:image' content='' />
<meta property='og:description' content='' />
<meta property='og:url' content='https://github.com/' />
<meta property='og:image:width' content='1200' />
<meta property='og:image:height' content='663' />
<!-- TYPE BELOW IS PROBABLY: 'website' or 'article' or look on https://ogp.me/#types -->
<meta property="og:type" content='website' />

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-9VZKE74FPW"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-9VZKE74FPW');
  </script>
  <meta charset="utf-8">
  <meta name="description" content="VLDBench: Vision Language Models Disinformation Detection Benchmark">
  <meta name="keywords" content="Disinformation Detection Benchmark">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VLDBench: Vision Language Models Disinformation Detection Benchmark</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/tab_gallery.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="juxtapose/css/juxtapose.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/magnifier.js"></script>
  <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/image_card_fader.css">
  <link rel="stylesheet" href="./static/css/image_card_slider.css">

</head>

<style>
  @import url('https://fonts.cdnfonts.com/css/menlo');
</style>


<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">VLDBench: <u>V</u>ision <u>L</u>anguage Models
              <u>D</u>isinformation Detection <u>Bench</u>mark
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><a href="https://scholar.google.com/citations?user=chcz7RMAAAAJ&hl=en">Shaina Raza</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://ashmalvayani.github.io/">Ashmal Vayani</a><sup>2</sup>,</span>
              <span class="author-block"><a href="https://www.linkedin.com/in/adityajain93/">Aditya
                  Jain</a><sup>3</sup>,</span>
              <span class="author-block"><a href="https://aravind-3105.github.io/">Aravind Narayanan</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=lEWvRbIAAAAJ&hl=en">Vahid
                  Reza Khazaie</a><sup>1</sup></span>
              <span class="author-block"><a href="https://scholar.google.ca/citations?user=BjdcXvUAAAAJ&hl=en">Syed Raza
                  Bashir</a><sup>4</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.ca/citations?user=A2AROnAAAAAJ&hl=en">Elham
                  Dolatabad</a><sup>5</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=6GXJI08AAAAJ&hl=en">Gias Uddin</a><sup>5</sup>,</span>
              <span class="author-block"><a
                  href="https://scholar.google.co.uk/citations?user=FZRIusYAAAAJ&hl=en">Christos
                  Emmanouilidis</a><sup>6</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=7j9pveMAAAAJ&hl=en">Rizwan
                  Qureshi</a><sup>2</sup>,</span>
              <span class="author-block"><a
                  href="https://scholar.google.com/citations?user=p8gsO3gAAAAJ&hl=en&oi=ao">Mubarak
                  Shah</a><sup>2</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Vector Institute for Artificial Intelligence</span>
              <span class="author-block"><sup>2</sup>University of Central Florida</span>
              <span class="author-block"><sup>3</sup>The University of Texas at Austin</span>
              <span class="author-block"><sup>4</sup>Sheridan College</span>
              <span class="author-block"><sup>5</sup>York University</span>
              <span class="author-block"><sup>6</sup>University of Groningen</span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://www.arxiv.org/abs/2502.11361" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/vector-institute/VLDBench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/VectorInstitute/VLDBench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            
                <span class="link-block">
                  <a href="#bibtex" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-obp"></i>
                    </span>
                    <span>BibTex</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
<div class="container is-max-desktop">
  <div class="hero-body">
    <p align="justify">
      Disinformation in the age of generative AI is no longer confined to isolated claims or doctored images; it increasingly manifests as complex, multimodal narratives that mimic credible journalism. 
      <b><i>VLDBench</i></b> addresses this challenge by offering the first large-scale, human-verified benchmark for disinformation detection spanning both text-only and image-text formats. 
      Built in alignment with global AI governance standards, it serves as a resource for evaluating model reliability, robustness, and interpretability across real-world scenarios.
    </p>

 

    <div class="has-text-centered" style="margin: 20px 0;">
  <img src="static/images/teaser.jpg" alt="VLDBench diagram" style="max-width: 20%; height: auto;">
</div>

    <p align="justify">
      To address this, we introduce <b>VLDBench</b>.
    </p>

    <div class="column">
      <div style="text-align: center;">
        <img src="static/images/Framework.jpg" alt="VLDBench Framework" style="max-width: 100%;">
      </div>

      <div class="content has-text-justified">
        <p align="justify">
          <b><span>Figure</span></b>: 
          <i>VLDBench Framework</i> system comprises five stages: 
          (1) Define Task – formalizing the detection objective; 
          (2) Data Pipeline – curating and preprocessing real-world multimodal news content; 
          (3) Annotation Pipeline – generating labels via human and LLM-assisted review; 
          (4) Human Review – validating annotations through expert oversight; and 
          (5) Benchmarking – evaluating models for accuracy, reasoning, and risk mitigation across fine-tuning, zero-shot, and robustness scenarios.
        </p>
      </div>
    </div>

    <br><br>
  </div>
</div>


  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container is-max-desktop has-text-centered">
        <!-- Visual Effects. -->
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            <p>
              <p>
The rise of AI-generated content has amplified the challenge of detecting multimodal disinformation—i.e., online posts/articles that contain images and texts with fabricated information, is specially designed to deceive. While prior AI safety benchmarks focus on bias and toxicity, multimodal disinformation detection remains underexplored.
To address this challenge, we present the Vision-Language Disinformation Detection Benchmark \textbf{(VLDBench)}, the first comprehensive benchmark for detecting disinformation across both unimodal (text-only) and multimodal (text and image) content, comprising 31,000 news article-image pairs, spanning 13 distinct categories, for robust evaluation. \textbf{VLDBench} features a rigorous semi-automated data curation pipeline, with 22 domain experts dedicating 300+ hour\textbf{s} to annotate the entire 31k samples, achieving a strong inter-annotator agreement (Cohen’s $\kappa = 0.78$).  
We extensively evaluate state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs), demonstrating that integrating textual and visual cues in multimodal news posts improves disinformation detection accuracy by 5–35\% compared to unimodal models. Developed in alignment with AI governance frameworks such as the EU AI Act, NIST guidelines, and the MIT AI Risk Repository 2024, \textbf{VLDBench} is expected to become a benchmark for detecting disinformation in online multi-modal contents.                 </p>
                <br>
          </p>
        </div>
      </div>
    </div>


  </section>




  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p>
            <h5> <b> Main contributions: </b></h5>
            <ol>

              <li> <b>VLDBench: </b>We present VLDBench, the largest human-verified benchmark for disinformation detection in both unimodal and multimodal settings. It contains 31.3K news articles with paired images, sourced from 58 outlets and spanning 13 categories, all collected under rigorous ethical guidelines.
              <li><b>Task coverage: </b>VLDBench offers 62K labeled instances supporting two evaluation formats: (i) binary classification for text or text-image pairs and (ii) open-ended multimodal reasoning.</li>
              <li><b>Expert annotation quality: </b>Twenty-two domain experts devoted more than 500 hours to annotation, yielding substantial agreement (Cohen’s κ = 0.78) and ensuring high data reliability.</li>
              <li><b>Comprehensive benchmarking: </b>We evaluate nineteen state-of-the-art open-source models (ten vision–language models and nine language models) on VLDBench, exposing systematic performance gaps and model-specific failure modes that inform responsible-AI governance and risk monitoring.</li>
            </ol>
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

    </div>
  </section>

  <section class="section">

    <!--/ Matting. -->
    <div class="container is-max-desktop">

      <!-- Latent space editing applications -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          
          <h2 class="title is-3 has-text-centered">VLDBench Dataset Overview</h2>

          <div class="content has-text-justified">
            <p>VLDBench is a comprehensive classification multimodal benchmark for disinformation detection in news articles. We categorized our data into 13 unique news categories by providing image-text pairs to GPT-4o.</p>
            <div class="content has-text-centered"><img src="static/images/DatasetComparison.jpg"  style="max-width:80%">
              <p class="content has-text-justified"> <b> <span>Table</span></b>: Comparison of <strong>VLDBench</strong> with contemporary datasets. The <em>Annotation</em> (<span style="color:blue;">👤</span>) means Manual and (<span style="color:blue;">👤</span>,<span style="color:#c2185b;">⚙️</span>) indicates Hybrid (Human+AI).
                <em>Access</em> (<span style="color:green;">✔️</span>) refers to open-source and (<span style="color:red;">❗</span>) indicates Request required. The <em>Real</em> is defined as (<span style="color:green;">✔️</span>) if it’s real-world data and (<span style="color:red;">❌</span>) if it’s Synthetic data.
                <sup>*</sup><strong>Multiple</strong> includes <em>Politics, National, Business &amp; Finance, International, Local/Regional, Entertainment, Opinion/Editorial, Health, Other, Sports, Technology, Weather &amp; Environment, Science</em>.
              </p>
            </div>
            
            <!-- <br><br> -->
          </div>
          <!-- <div class="content has-text-justified"> -->
            <!-- <p> -->
              <!-- <b> <span style="color: blue;">SB-Bench</span></b> comprises of nine social bias categories. -->
            <!-- <div class="content has-text-centered">
              <img src="static/images/news_categories_distribution_page-0001.jpg" style="max-width:80%">
              <p class="content has-text-justified"><b> <span>Figure</span></b>: Category distribution with overlaps. Total unique articles = 31,339. Percentages sum to > 100% due to multi-category articles.</p> -->
              <!-- <img src="static/images/outlet_distribution.jpg" style="max-width:100%">
              <p class="content has-text-justified">
                Distribution of Articles Across Various News Outlets. This bar chart illustrates the number
                of articles published by each outlet in our dataset, highlighting the predominant sources of news
                coverage.
              </p> -->
            <!-- </div> -->
          <!-- </div> -->
          

        </div>
      </div>

      <!--/ Matting. -->
      <div class="container is-max-desktop">

        <!-- Latent space editing applications -->
        <div class="columns is-centered">
          <div class="column is-full-width">
            <!-- <h2 class="title is-3 has-text-centered">VLDBench Pipeline</h2> -->
            <div class="content has-text-centered">
              <!-- <img src="static/images/architecture.png" style="max-width:100%">
              <p align="justify"> <b> <span>Figure</span></b>:
                <i>VLDBench</i> is a multimodal disinformation detection framework, focusing on LLM/VLM benchmarking,
                human-AI collaborative annotation, and risk mitigation. It operates through a three-stage
                pipeline: (1) Data (collection, filtering, and quality assurance of text-image pairs), (2) Annotation
                (GPT4 labeling with human validation), (3) Benchmarking (prompt-based evaluation and robustness
                testing).
              </p> -->

             


              <!-- <img src="./static/images/paired_image_pipeline.jpg" style="max-width:50%">
            
              <p class="content has-text-justified">
              <div class="content has-text-justified">
                <p align="justify"> <b> <span>Figure</span></b>:
                  Paired Images Pipeline: For dual-image queries, the Dual Query Generator creates two separate queries, each
                  independently sent to a web search. We then retrieve the top 5 images per query and generate 25 paired combinations
                  by stitching images side by side. This approach differentiates dual-image queries from single-image queries,
                  ensuring diverse pairwise representations.</p>
              </div> -->

<!-- 
              <div class="content has-text-justified">
                <h3 class="title is-4 has-text-justified">Data Statistics</h3>
              </div> -->


              <div class="content has-text-justified">
                <div class="content has-text-centered">
                  <img src="static/images/SummaryStats.jpg" style="max-width:70%">
                </div>
                <p align="justified"> <b> <span>Figure</span></b>: Summary statistics for VLDBench. Each article is annotated twice, once as text-only and once as text +image, yielding 62,678 labelled instances.</p>
              </div>
            </div>

            <div class="content has-text-justified">
              <div class="content has-text-centered">
                <img src="static/images/qualitative_page-0001.jpg" style="max-width:80%">
              </div>
              <p align="justified"> <b> <span>Figure</span></b>: Disinformation Trends Across News Categories generated by GPT-4o based on disinformation narratives and confidence levels. </p>
              </p>
            </div>
          </div>

        </div>
      </div>
      <br><br>
      <div class="content has-text-centered">
        <h3 class="title is-4 has-text-justified">LLMs and VLMs used for Benchmarking</h3>
        <div class="content has-text-justified">
          <table style="width:80%" align="center">
            <tr>
                <th>Language-Only LLMs</th>
                <th>Vision-Language Models (VLMs)</th>
            </tr>
        
            <tr>
                <td>Phi-3-mini-128k-instruct</td>
                <td>Phi-3-Vision-128k-Instruct</td>
            </tr>
            <tr>
                <td>Vicuna-7B-v1.5</td>
                <td>LLaVA-v1.5-Vicuna7B</td>
            </tr>
            <tr>
                <td>Mistral-7B-Instruct-v0.3</td>
                <td>LLaVA-v1.6-Mistral-7B</td>
            </tr>
            <tr>
                <td>Qwen2-7B-Instruct</td>
                <td>Qwen2-VL-7B-Instruct</td>
            </tr>
            <tr>
                <td>InternLM2-7B</td>
                <td>InternVL2-8B</td>
            </tr>
            <tr>
                <td>DeepSeek-V2-Lite-Chat</td>
                <td>Deepseek-VL2-small</td>
            </tr>
            <tr>
                <td>GLM-4-9B-chat</td>
                <td>GLM-4V-9B</td>
            </tr>
            <tr>
                <td>LLaMA-3.1-8B-Instruct</td>
                <td>LLaMA-3.2-11B-Vision </td>
            </tr>
            <tr>
                <td>LLaMA-3.2-1B-Instruct</td>
                <td>Deepseek Janus-Pro-7B</td>
            </tr>
            <tr>
              <td></td>
              <td>Pixtral</td>
          </tr>
        </table>
        
        
        <!-- <br>
        <div class="content has-text-justified">
          <p align="justify"> <b> <span>Table</span></b>: We evaluate the standard deviation for Qwen2-VL-7B and InternVL2-8B
            models on randomized multiple-choice orders and shuffled images in the paired image setting. Both models exhibit
            low variability and are consistent. </p>
        </div>
        <br> -->
      </div>

      <br><br>

      <div class="columns is-centered">
        <div class="column is-full-width has-text-centered">
          <h2 class="title is-3">Experimental results on VLDBench</h2>

          <!-- <h3 class="title is-4 has-text-justified">Multimodal Models Surpass Unimodal Baselines</h3> -->

          <!-- <div class="content has-text-centered"> -->

            <!-- <img src="static/images/zero-shot_performance.png" style="max-width:100%"> -->
             <!-- <img src="static/images/radar_plot_page-0001.jpg" style="max-width:80%"> -->
            <!-- <p style="text-align: justify;"> <b> <span>Figure</span></b>: Performance comparison of vision language models for disinformation detection across key metrics: precision, recall, F1 and accuracy, with different colors representing distinct metrics </p> -->
          <!-- </div> -->
          <!-- <br /> -->
          <h3 class="title is-4 has-text-justified">Instruction Fine-Tuning on <b>VLDBench</b> Improves Detection Performance</h3>
          <div class="content has-text-centered">
            <img src="static/images/ift.png" style="max-width:80%">
            <p style="text-align: justify;"> <span><b>Figure</b></span>: Comparison of zero-shot vs. instruction-fine-tuned (IFT) performance, with 95% confidence intervals computed from three independent runs. </p>


          </div>
        </div>
      </div>

      <h3 class="title is-4 has-text-justified">Adversarial Robustness: Combined Modality is More Vulnerable</h3>
      <!-- <h4 class="title is-4 has-text-justified">Text and Image Attacks</h4> -->

      <div class="content has-text-justified">
        <div class="content has-text-centered">
          <img src="static/images/perturbation_text_page-0001.jpg" style="max-width:50%">
          <p style="text-align: justify;"> <b> <span>Figure</span></b>: Textual Perturbations. We describe three controlled text perturbations—<i>Synonym Substitution, Misspelling, and Negation</i>—and analyse how each distorts meaning. Our evaluation shows that text negation flips factual statements most often into disinformation, driving the largest drop in model accuracy</p>
        </div>

          
        <!-- <p>Image perturbations (I-P) include blurring, gaussian noise, and resizing. We also include Multi modality attacks: cross-modal misalignment (C-M) (e.g., mismatched image captions) and both modality perturbations (B-P) (both text and image distortions). The following image is an example of image, cross-modal, and both-modality perturbations.
        </p> -->
        <div class="content has-text-centered">
          <img src="static/images/perturbation_image_page-0001.jpg" style="max-width:50%">
          <p style="text-align: justify;"> <b> <span>Figure</span></b>: Visual Perturbations. We introduce five image attacks—<i>Gaussian Blur, Additive Noise, Resizing, Cross-Modal Mismatch (C-M), and Both-Modality (B-M)</i>—and report that the cross-modal and combined attacks cause the greatest misclassification under multimodal setting.</p>
        </div>
        
      </div>
     

      

      <h3 class="title is-4 has-text-justified">Human Evaluation Demonstrates Reliability and Reasoning Depth</h3>

      <div class="content has-text-centered">
        <img src="static/images/reasoning_page-0001.jpg" style="max-width:80%">
        <p style="text-align: justify;"> <b> <span>Figure</span></b>: Human evaluation results on a 500-sample test set. Models were tasked with classifying disinformation and justifying their predictions. <strong>PC</strong> = prediction correctness, <strong>RC</strong> = reasoning clarity (mean &plusmn; std.).
        </p>
      </div>

      <h3 class="title is-4 has-text-justified">AI Risk Mitigation and Governance Alignment</h3>
      <div class="content has-text-centered">
        <img src="static/images/AIAlignment.jpg" style="max-width:70%">
        <p style="text-align: justify;"> <b> <span>Figure</span></b>: Mapping of VLDBench components to risk mitigation strategies outlined in the AI MIT Risk and Responsibility Repository. Each pipeline component addresses specific risks related to privacy, misinformation, discrimination, robustness, and interpretability.
        </p>
      </div>

      <div class="content has-text-justified">
        <h3 class="title is-4 has-text-justified">Conclusion</h3>
        <div class="content has-text-justified">
          <p>
            <b><i>VLDBench</i></b> addresses the urgent challenge of AI-era disinformation through responsible data stewardship, prioritizing human-centered design, ethical data sourcing, and governance-aligned evaluation (EU AI Act, NIST). Unlike existing benchmarks, it is the first to explicitly evaluate modern LLMs/ VLMs on disinformation topic, with 62k multimodal samples spanning 13 topical categories (e.g., sports, politics). While compatible with traditional ML models, its design focuses on emerging multimodal threats. Some limitations need discussion : (1) reliance on pre-verified news sources risks sampling bias, (2) hybrid AI-human annotations may inherit annotator biases, and (3) the English-only corpus limits multilingual applicability. Future work should expand to adversarial cross-modal attacks (e.g., deepfake text-image contradictions) and low-resource languages. Despite these constraints, <b><i>VLDBench</i></b> establishes a foundational step toward systematic disinformation benchmarking, enabling researchers to stress-test models agains real-world deception tactics while adhering to AI governance frameworks.</p>
          <br>
          <p>For additional details about VLDBench evaluation and experimental results, please refer to our main paper.
            Thank you! </p>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title"><a id="bibtex">BibTeX</a></h2>
      <pre><code>@misc{raza2025vldbenchvisionlanguagemodels,
        title={VLDBench: Vision Language Models Disinformation Detection Benchmark}, 
        author={Shaina Raza and Ashmal Vayani and Aditya Jain and Aravind Narayanan and Vahid Reza Khazaie and Syed Raza Bashir and Elham Dolatabadi and Gias Uddin and Christos Emmanouilidis and Rizwan Qureshi and Mubarak Shah},
        year={2025},
        eprint={2502.11361},
        archivePrefix={arXiv},
        primaryClass={cs.CL},
        url={https://arxiv.org/abs/2502.11361}, 
  }
</code></pre>
    </div>
  </section>


<!--   <footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p class="has-text-centered">
            Website adapted from the following <a href="https://mingukkang.github.io/GigaGAN/">source code</a>.
          </p>
        </div>
      </div>
    </div>
    </div>
  </footer> -->


  <script src="juxtapose/js/juxtapose.js"></script>

  <script>
    var slider;
    let origOptions = {
      "makeResponsive": true,
      "showLabels": true,
      "mode": "horizontal",
      "showCredits": true,
      "animate": true,
      "startingPosition": "50"
    };

    const juxtaposeSelector = "#juxtapose-embed";
    const transientSelector = "#juxtapose-hidden";

    inputImage.src = "./static/images/".concat(name, "_input.jpg")
    outputImage.src = "./static/images/".concat(name, "_output.jpg")

    let images = [inputImage, outputImage];
    let options = slider.options;
    options.callback = function (obj) {
      var newNode = document.getElementById(obj.selector.substring(1));
      var oldNode = document.getElementById(juxtaposeSelector.substring(1));
      console.log(obj.selector.substring(1));
      console.log(newNode.children[0]);
      oldNode.replaceChild(newNode.children[0], oldNode.children[0]);
      //newNode.removeChild(newNode.children[0]);

    };

    slider = new juxtapose.JXSlider(transientSelector, images, options);
};



    (function () {
      slider = new juxtapose.JXSlider(
        juxtaposeSelector, origImages, origOptions);
      //document.getElementById("left-button").onclick = replaceLeft;
      //document.getElementById("right-button").onclick = replaceRight;
    })();
    // Get the image text
    var imgText = document.getElementById("imgtext");
    // Use the same src in the expanded image as the image being clicked on from the grid
    // expandImg.src = imgs.src;
    // Use the value of the alt attribute of the clickable image as text inside the expanded image
    imgText.innerHTML = name;
    // Show the container element (hidden with CSS)
    // expandImg.parentElement.style.display = "block";

    $(".flip-card").click(function () {
      console.log("fading in")
      div_back = $(this).children().children()[1]
      div_front = $(this).children().children()[0]
      // console.log($(this).children("div.flip-card-back"))
      console.log(div_back)
      $(div_front).addClass("out");
      $(div_front).removeClass("in");

      $(div_back).addClass("in");
      $(div_back).removeClass("out");

    });

    $(".flip-card").mouseleave(function () {
      console.log("fading in")
      div_back = $(this).children().children()[1]
      div_front = $(this).children().children()[0]
      // console.log($(this).children("div.flip-card-back"))
      console.log(div_back)
      $(div_front).addClass("in");
      $(div_front).removeClass("out");

      $(div_back).addClass("out");
      $(div_back).removeClass("in");

    });

  </script>
  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script> -->
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js"
    integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>

</body>

</html>
