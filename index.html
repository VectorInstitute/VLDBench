<!DOCTYPE html>
<html>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<meta property='og:title' content='VLDBench: Vision Language Models Disinformation Detection Benchmark' />
<meta property='og:image' content='' />
<meta property='og:description' content='' />
<meta property='og:url' content='https://github.com/' />
<meta property='og:image:width' content='1200' />
<meta property='og:image:height' content='663' />
<!-- TYPE BELOW IS PROBABLY: 'website' or 'article' or look on https://ogp.me/#types -->
<meta property="og:type" content='website' />

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-9VZKE74FPW"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-9VZKE74FPW');
  </script>
  <meta charset="utf-8">
  <meta name="description" content="SB-Bench: Stereotype Bias Benchmark for Large Multimodal Models">
  <meta name="keywords" content="Stereotype-Bias Benchmark">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VLDBench: Vision Language Models Disinformation Detection Benchmark</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/tab_gallery.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="juxtapose/css/juxtapose.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/magnifier.js"></script>
  <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/image_card_fader.css">
  <link rel="stylesheet" href="./static/css/image_card_slider.css">

</head>

<style>
  @import url('https://fonts.cdnfonts.com/css/menlo');
</style>


<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">VLDBench: <u>V</u>ision <u>L</u>anguage Models
              <u>D</u>isinformation Detection <u>Bench</u>mark
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><a href="https://scholar.google.com/citations?user=chcz7RMAAAAJ&hl=en">Shaina Raza</a><sup>1</sup><sup>*</sup>,</span>
              <span class="author-block"><a href="https://ashmalvayani.github.io/">Ashmal Vayani</a><sup>2</sup>,</span>
              <span class="author-block"><a href="https://www.linkedin.com/in/adityajain93/">Aditya
                  Jain</a><sup>3</sup>,</span>
              <span class="author-block"><a href="https://swetha5.github.io/">Aravind Narayanan</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=lEWvRbIAAAAJ&hl=en">Vahid
                  Reza Khazaie</a><sup>1</sup></span>
              <span class="author-block"><a href="https://scholar.google.ca/citations?user=BjdcXvUAAAAJ&hl=en">Syed Raza
                  Bashir</a><sup>4</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.ca/citations?user=A2AROnAAAAAJ&hl=en">Elham
                  Dolatabad</a><sup>5</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=6GXJI08AAAAJ&hl=en">Gias Uddin</a><sup>5</sup>,</span>
              <span class="author-block"><a
                  href="https://scholar.google.co.uk/citations?user=FZRIusYAAAAJ&hl=en">Christos
                  Emmanouilidis</a><sup>6</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=7j9pveMAAAAJ&hl=en">Rizwan
                  Qureshi</a><sup>2</sup>,</span>
              <span class="author-block"><a
                  href="https://scholar.google.com/citations?user=p8gsO3gAAAAJ&hl=en&oi=ao">Mubarak
                  Shah</a><sup>2</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Vector Institute for Artificial Intelligence</span>
              <span class="author-block"><sup>2</sup>University of Central Florida</span>
              <span class="author-block"><sup>3</sup>The University of Texas at Austin</span>
              <span class="author-block"><sup>4</sup>Sheridan College</span>
              <span class="author-block"><sup>5</sup>York University</span>
              <span class="author-block"><sup>6</sup>University of Groningen</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>*</sup>Corresponding author:
                <a href="mailto:shaina.raza@vectorinstitute.ai">shaina.raza@vectorinstitute.ai</a></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/pdf/2502.08779" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                </span> -->
                <!-- <span class="link-block">
                  <a href="https://huggingface.co/datasets/ucf-crcv/SB-Bench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span> -->
                <span class="link-block">
                  <a href="#" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            
                <!-- <span class="link-block">
                  <a href="#bibtex" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-obp"></i>
                    </span>
                    <span>BibTex</span>
                  </a>
                </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <p align="justify">
          Motivated by the growing influence of Generative AI in shaping digital narratives and the critical need to combat disinformation, we present the Vision-Language Disinformation Detection Benchmark <b><i>(VLDBench)</i></b>. This comprehensive benchmark empowers researchers to evaluate and enhance the capabilities of AI systems in detecting multimodal disinformation, addressing the unique challenges posed by the interplay of textual and visual content. By bridging gaps in existing benchmarks, VLDBench sets the stage for building safer, more transparent, and equitable AI models that safeguard public trust in digital platforms.</p>
        <!-- <br> -->

        <div class="column">
          <div style="text-align:center;">
            <!-- <h4 class="subtitle has-text-centered"> -->
            <img src="static/images/architecture.png">
            <!-- </h4> -->

            <div class="content has-text-justified">
              <p align="justify"> <b> <span>Figure</span></b>:
                <i>VLDBench</i> is a multimodal disinformation detection framework, focusing on LLM/VLM benchmarking,
                human-AI collaborative annotation, and risk mitigation. It operates through a three-stage
                pipeline: (1) Data (collection, filtering, and quality assurance of text-image pairs), (2) Annotation
                (GPT4 labeling with human validation), (3) Benchmarking (prompt-based evaluation and robustness
                testing).
              </p>
            </div>
          </div>
        </div>

        <br><br>
      </div>
    </div>

  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container is-max-desktop has-text-centered">
        <!-- Visual Effects. -->
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The rapid rise of Generative AI (GenAI)-generated content has made detecting disinformation increasingly
            challenging. In particular, multimodal disinformation—deliberately false information designed to deceive—has
            become a pressing problem on digital platforms. While existing AI safety benchmarks primarily address bias
            and toxicity, disinformation detection remains largely underexplored. To address this challenge, we present
            the Vision-Language Disinformation Detection Benchmark <i>(VLDBench)</i>—the first comprehensive benchmark
            for detecting disinformation across both unimodal (text-only) and multimodal (text and image) content,
            comprising <i>31,000</i> news article-image pairs, spanning 13 distinct categories, for robust evaluation.
            <i>VLDBench</i> features a rigorous semi-automated data curation pipeline, with 22 domain experts dedicating
            300+ hours to annotation, achieving a strong inter-annotator agreement (Cohen’s κ = 0.82). We extensively
            evaluate state-of-the-art Large Language Models (LLMs) and Vision Language Models (VLMs), demonstrating that
            integrating textual and visual cues in multimodal news posts improves disinformation detection accuracy by
            5–15% compared to unimodal models. Developed in alignment with AI governance frameworks such as the EU AI
            Act, NIST guidelines, and the MIT AI Risk Repository 2024, <i>VLDBench</i> is expected to become a benchmark
            for detecting disinformation in online multi-modal content. Our code and data will be made publicly
            available.<br>
          </p>
        </div>
      </div>
    </div>


  </section>




  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3"><i>VLDBench</i> is the largest and most comprehensive humanly verified disinformation
            detection benchmark with over 300 hours of human verification.</h2>
          <div class="content has-text-justified">
            <p>
            <h5> <b> Main contributions: </b></h5>
            <ol>
              <li> <b>VLDBench: </b> A comprehensive, human-verified multimodal benchmark for disinformation detection,
                featuring 31.3k news article-image pairs across 13 categories.</li>
              <li><b>Expert Annotation: </b>Curated by 22 domain experts over 300+ hours, achieving high label accuracy
                (Cohen’s κ = 0.82).</li>
              <li><b>Model Benchmarking: </b>Evaluates LLMs and VLMs, identifying strengths and weaknesses in multimodal
                disinformation detection.</li>
              <li><b>Open-Source Initiative: </b>Releases
                136 datasets and training recipes to foster
                137 transparency, reproducibility, and further
                138 research.</li>

            </ol>
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

    </div>
  </section>

  <section class="section">

    <!--/ Matting. -->
    <div class="container is-max-desktop">

      <!-- Latent space editing applications -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          
          <h2 class="title is-3 has-text-centered">VLDBench Dataset Overview</h2>

          <div class="content has-text-justified">
            <p>VLDBench is a comprehensive classification multimodal benchmark for disinformation detection in news articles. We categorized our data into 13 unique news categories by providing image-text pairs to GPT-4o.</p>
            <img src="static/images/ComparisonDatasets.png"  style="max-width:100%">
            <br><br>
          </div>
          <div class="content has-text-justified">
            <p>
              <!-- <b> <span style="color: blue;">SB-Bench</span></b> comprises of nine social bias categories. -->
            <div class="content has-text-centered">
              <img src="static/images/news_categories_distribution_page-0001.jpg" style="max-width:100%">
              <p class="content has-text-justified">
                Category distribution with overlaps. Total unique articles = 31,339. Percentages sum to > 100% due to
                multi-category articles.
              </p>
              <!-- <img src="static/images/outlet_distribution.jpg" style="max-width:100%">
              <p class="content has-text-justified">
                Distribution of Articles Across Various News Outlets. This bar chart illustrates the number
                of articles published by each outlet in our dataset, highlighting the predominant sources of news
                coverage.
              </p> -->
            </div>
>
          </div>
          

        </div>
      </div>

      <!--/ Matting. -->
      <div class="container is-max-desktop">

        <!-- Latent space editing applications -->
        <div class="columns is-centered">
          <div class="column is-full-width">
            <!-- <h2 class="title is-3 has-text-centered">VLDBench Pipeline</h2> -->
            <div class="content has-text-centered">
              <!-- <img src="static/images/architecture.png" style="max-width:100%">
              <p align="justify"> <b> <span>Figure</span></b>:
                <i>VLDBench</i> is a multimodal disinformation detection framework, focusing on LLM/VLM benchmarking,
                human-AI collaborative annotation, and risk mitigation. It operates through a three-stage
                pipeline: (1) Data (collection, filtering, and quality assurance of text-image pairs), (2) Annotation
                (GPT4 labeling with human validation), (3) Benchmarking (prompt-based evaluation and robustness
                testing).
              </p> -->

             


              <!-- <img src="./static/images/paired_image_pipeline.jpg" style="max-width:50%">
            
            <p class="content has-text-justified">
            <div class="content has-text-justified">
              <p align="justify"> <b> <span>Figure</span></b>:
                Paired Images Pipeline: For dual-image queries, the Dual Query Generator creates two separate queries, each
                independently sent to a web search. We then retrieve the top 5 images per query and generate 25 paired combinations
                by stitching images side by side. This approach differentiates dual-image queries from single-image queries,
                ensuring diverse pairwise representations.</p>
            </div> -->


              <div class="content has-text-justified">
                <h3 class="title is-4 has-text-justified">Data Statistics</h3>

                <p align="justify"> Data comprises 31,339 articles and visual samples curated from 58 news sources
                  ranging
                  from the Financial Times, CNN, and New York Times to Axios and Wall Street Journal. VLDBench spans 13
                  unique categories: National, Business and Finance,International, Entertainment, Local/Regional,
                  Opinion/Editorial, Health, Sports, Politics, Weather and Environment, Technology, Science, and Other
                  —adding depth to the disinformation domains. </p>
              </div>


              <div class="content has-text-justified">
                <div class="content has-text-centered">
                  <img src="static/images/key_dataset_statistics.png" style="max-width:60%">
                </div>
                <p align="justified"> <b> <span>Figure</span></b>: Key Dataset Statistics</p>
              </div>
            </div>


            <div class="content has-text-justified">
              <h3 class="title is-4 has-text-justified">Annotation Pipeline</h3>
            
              <p align="justify"> VLDBench represents the largest and most comprehensive human-verified disinformation detection
                benchmark, with over 300 hours of human verification. Using a semi-automated pipeline, each article was categorized
                by GPT-4o as either "Likely" or "Unlikely" to contain disinformation. This binary categorization ensures a balance
                between nuanced evaluation and manageability.
                <br><br>
                To enhance reliability, GPT-4o evaluated text-image alignment three times per sample to minimize variance and
                resolve potential ties in classification. Its selection was based on demonstrated effectiveness in both textual and
                visual reasoning tasks. The annotations were systematically reviewed by 22 domain experts, who assessed accuracy,
                consistency, and alignment with human judgment. This rigorous process resulted in strong inter-annotator agreement
                with a Cohen’s κ of 0.82.
                <br><br>
                The figure below provides an example of disinformation narratives analyzed by GPT-4o, illustrating confidence levels
                and reasoning. </p>
            </div>

            


            <div class="content has-text-justified">
              <div class="content has-text-centered">
                <img src="static/images/qualitative_page-0001.jpg" style="max-width:100%">
              </div>
              <p align="justified"> <b> <span>Figure</span></b>: We analyze the likelihood of disinformation across
                different news categories, based on disinformation narratives and confidence levels generated by GPT-4o.
              </p>
            </div>
          </div>

        </div>
      </div>
      <br><br>
      <div class="content has-text-centered">
        <h3 class="title is-4 has-text-justified">LLMs and VLMs used for Evaluation</h3>
        <div class="content has-text-justified">
          <p align="justify"> We benchmark twelve state-of-the-art open-source VLMs and seven LLMs on VLDBench, evaluating LLMs on text-only tasks and VLMs on multimodal analysis (text + images). We focus on open-source LLMs and VLMs to promote accessibility and transparency in our research. Additionally, since GPT-4o is used for annotations in this study, evaluating open models allows for a more comprehensive comparison. The evaluation process includes both quantitative and qualitative assessments. Quantitative metrics—accuracy, precision, recall, and F1-score—are measured during evaluations based on both prompt-based and fine-tuned models </p>
        <table border="1" cellspacing="0" style="width: 400px; border-collapse: collapse; text-align: left; margin: auto;">
          <thead>
            <tr style="background-color:whitesmoke;">
              <th colspan="1" align="center" style="font-size: 13px;">Model Type</th>
              <th colspan="1" align="center" style="font-size: 13px;">Model Name</th>
            </tr>
          </thead>
          <tbody>
            <tr style="background-color:beige;">
              <td colspan="1" rowspan="7" style="font-size: 13px; text-align: center; vertical-align: middle;">Language-Only LLMs</td>
              <td colspan="1" style="font-size: 13px;">Phi-3-mini-128k-instruct</td>
            </tr>
            <tr style="background-color:whitesmoke;">
              <td colspan="1" style="font-size: 13px;">Vicuna-7b-v1.5</td>
            </tr>
            <tr style="background-color:beige;">
              <td colspan="1" style="font-size: 13px;">Mistral-7B-Instruct-v0.3</td>
            </tr>
            <tr style="background-color:whitesmoke;">
              <td colspan="1" style="font-size: 13px;">Qwen2-7B-Instruct</td>
            </tr>
            <tr style="background-color:beige;">
              <td colspan="1" style="font-size: 13px;">Llama-3.1-8B-Instruct</td>
            </tr>
            <tr style="background-color:whitesmoke;">
              <td colspan="1" style="font-size: 13px;">Llama-3.2-1B-Instruct</td>
            </tr>
            <tr style="background-color:beige;">
              <td colspan="1" style="font-size: 13px;">DeepSeek-V2-Lite-Chat</td>
            </tr>
            
            <tr style="background-color:beige;">
              <td colspan="1" rowspan="12" style="font-size: 13px; text-align: center; vertical-align: middle;">Vision-Language Models (VLMs)</td>
              <td colspan="1" style="font-size: 13px;">Phi-3-vision-128k-instruct</td>
            </tr>
            <tr style="background-color:whitesmoke;">
              <td colspan="1" style="font-size: 13px;">Deepseek Janus-Pro-7B</td>
            </tr>
            <tr style="background-color:beige;">
              <td colspan="1" style="font-size: 13px;">Deepseek-vl2-small</td>
            </tr>
            <tr style="background-color:whitesmoke;">
              <td colspan="1" style="font-size: 13px;">Llava-v1.5-vicuna7b</td>
            </tr>
            <tr style="background-color:beige;">
              <td colspan="1" style="font-size: 13px;">Llava-v1.6-mistral-7b</td>
            </tr>
            <tr style="background-color:whitesmoke;">
              <td colspan="1" style="font-size: 13px;">Pixtral</td>
            </tr>
            <tr style="background-color:beige;">
              <td colspan="1" style="font-size: 13px;">Qwen2-VL-7B-Instruct</td>
            </tr>
            <tr style="background-color:whitesmoke;">
              <td colspan="1" style="font-size: 13px;">Llama-3.2-11B-Vision</td>
            </tr>
            <tr style="background-color:beige;">
              <td colspan="1" style="font-size: 13px;">GLM-4V-9B</td>
            </tr>
            <tr style="background-color:whitesmoke;">
              <td colspan="1" style="font-size: 13px;">InternVL2-8B</td>
            </tr>
            <tr style="background-color:beige;">
              <td colspan="1" style="font-size: 13px;">Deepseek-vl2-small</td>
            </tr>
            <tr style="background-color:whitesmoke;">
              <td colspan="1" style="font-size: 13px;">Janus-Pro-7B</td>
            </tr>
          </tbody>
        </table>
        
        
        <!-- <br>
        <div class="content has-text-justified">
          <p align="justify"> <b> <span>Table</span></b>: We evaluate the standard deviation for Qwen2-VL-7B and InternVL2-8B
            models on randomized multiple-choice orders and shuffled images in the paired image setting. Both models exhibit
            low variability and are consistent. </p>
        </div>
        <br> -->
      </div>

      <br><br>

      <div class="columns is-centered">
        <div class="column is-full-width has-text-centered">
          <h2 class="title is-3">Experimental results on VLDBench</h2>

          <div class="content has-text-justified">
            <p> Our investigation focuses on three core ques404 tions: (1) Does multimodal (text+image) data improve
              disinformation detection compared to text alone? (2) Does instruction-based fine tuning enhance
              generalization and robustness? (3) How vulnerable are models to adversarial perturbations in text and
              images? </p>
          </div>

          <h3 class="title is-4 has-text-justified">Comparison of Unimodality and Multimodality</h3>

          <div class="content has-text-centered">
            <p style="text-align: justify;">
              The below table shows that VLMs consistently outperform unimodal LLMs. For example, Llava-v1.6-Mistral-7B
              achieves an F1 of 69.89% vs. 66.56% from its unimodal counterpart, Mistral-7B. Similarly, <b>Pixtral
                obtains the highest F1 (70.70%)</b>, clearly exceeding that of language-only models. Across different
              model families (e.g., Phi-3 vs. Phi-3-vision, Qwen2 vs. Qwen2-VL), the <b>multimodal variants yield
                stronger detection rates</b>. These results reinforce the benefit of incorporating both textual and
              visual cues for disinformation detection</p>
            <!-- <img src="static/images/zero-shot_performance.png" style="max-width:100%"> -->
             <img src="static/images/radar_plot_page-0001.jpg" style="max-width:100%">
            <p style="text-align: justify;"> <b> <span>Figure</span></b>: Performance comparison of vision language models for disinformation detection across key metrics: precision, recall, F1 and accuracy, with different colors representing distinct metrics </p>


          </div>
          <br />
          <h3 class="title is-4 has-text-justified">Instruction Fine-Tuning Enhances Performance</h3>
          <div class="content has-text-centered">
            <p style="text-align: justify;">
              Instruction fine-tuning (IFT) on models like Phi, Mistral-Llava, Qwen, and Llama3.2—along with their vision-language counterparts—using the VLDBench training subset led to significant performance improvements. For instance, Phi-3-Vision-IFT achieved a 7% improvement over its base model, with similar gains observed in the text-only Phi-3-mini-128k model. These improvements highlight the models’ ability to adapt to disinformation-specific cues from VLDBench, showcasing the dataset's quality and effectiveness.</p>
            <img src="static/images/ift.png" style="max-width:100%">
            <p style="text-align: justify;"> Comparison of zero-shot vs. instruction-fine-tuned (IFT) performance, with 95% confidence intervals computed from three independent runs. </p>


          </div>
        </div>
      </div>

      <h2 class="title is-3">Robustness to Adversarial Perturbations</h2>
      <h3 class="title is-4 has-text-justified">Text and Image Attacks</h3>

      <div class="content has-text-justified">
        <p> We evaluated models under controlled adversarial conditions, such as textual synonyms, misspellings, and
          image modifications (blurring/resizing). Results showed that Vision-Language Models (VLMs) handle
          single-modality attacks effectively but struggle with cross-modal mismatches and combined perturbations.
          Language-Only Models (LLMs) showed higher vulnerability to text changes, with significant performance drops
          (e.g., LLama3.2-1B saw a 9.44% accuracy loss under text negations). Large VLMs experienced severe
          degradation, with combined text-image attacks reducing accuracy by up to 26.4%. These findings highlight the
          need for improved robustness in multimodal systems.</p>
      </div>
      <div class="content has-text-centered">
        <img src="static/images/perturbation_text_page-0001.jpg" style="max-width:100%">
        <p style="text-align: justify;"> <b> <span>Figure</span></b>: We describe the text perturbations in the caption,
          introducing Synonym, Misspelling, and Negation. Our analysis shows that text negation leads to a majority of
          disinformation cases.</p>
      </div>

      <div class="content has-text-centered">
        <img src="static/images/perturbation_image_page-0001.jpg" style="max-width:100%">
        <p style="text-align: justify;"> <b> <span>Figure</span></b>: We describe the image perturbations in the caption, introducing Blur, Noise, and Resizing, Cross-Modal (C-M) Mismatch, Both-Modality (BM). Our analysis shows that C-M and B-M leads to a majority of disinformation cases.</p>
      </div>

      <h3 class="title is-4 has-text-justified">Combined Attacks</h3>

      <div class="content has-text-justified">
        <p> Combining text+image adversarial attacks can cause catastrophic performance drops in high-capacity models (LLaMA.2-11B: 26.4% decrease). These findings illustrate that multimodal methods, despite generally higher baseline accuracy, remain susceptible when adversaries deliberately target both modalities. Future work may explore more robust architectures or data augmentation to mitigate these coordinated attacks.</p>
      </div>
      <!-- <div class="content has-text-centered">
        <img src="static/images/VisualTransformations.png" style="max-width:100%">
        <p style="text-align: justify;"> <b> <span>Figure</span></b>: Visual transformations and their impact on disinformation classification. Noise and downscaling can shift model outputs from “Yes” to “No.”</p>
      </div> -->

      <h3 class="title is-4 has-text-justified">Human Evaluation Establishes Reliability and Reasoning Depth</h3>

      <div class="content has-text-justified">
        <p> To assess model performance, we conducted a human evaluation of three IFT VLMs (LlaMA-3.2-11B, Pixtral, LLaVA-v1.6) on a balanced 500-sample test set (250 disinformation, neutral). Each model classified each sample as disinformation or neutral and provided a rationale for its decision. Three independent reviewers, blinded to model identities, assessed the outputs based on two key criteria: Prediction Correctness (PC)—how well the model’s classification aligned with the ground truth on a scale of 1–5, and Reasoning Clarity (RC)—the clarity and coherence of the model’s explanation, also rated on a scale of 1–5.</p>
      </div>
      <div class="content has-text-centered">
        <img src="static/images/reasoning_page-0001.jpg" style="max-width:100%">
        <p style="text-align: justify;"> <b> <span>Figure</span></b>: Human evaluation results on a 500-sample test set. Models were tasked with classifying disinformation and justifying their predictions. PC = prediction correctness, RC = reasoning clarity (mean ± std.).</p>
      </div>

      <div class="content has-text-justified">
        <h3 class="title is-4 has-text-justified">Conclusion</h3>
        <div class="content has-text-justified">
          <p>
            VLDBench addresses the urgent challenge of disinformation through a design rooted in responsible data stewardship
            and human-centered principles, integrating best practices to meet key AI governance requirements. Unlike other
            benchmarks, VLDBench uniquely targets the complexity of disinformation in the post-ChatGPT era, where GenAI has
            amplified a lot of false information. It is the first dataset explicitly designed to evaluate modern V/LLMs on
            emerging disinformation challenges, maintaining a topical focus.
            <br><br>
            However, some limitations need attention. The reliance on pre-verified news sources introduces potential sampling
            bias, and the annotation process, partially based on AI, may inherit some biases. There is also a need for more
            research into adversarial attacks on multimodal performance. The current focus on English language only limits its applicability to multilingual and culturally diverse contexts. Despite these limitations, VLDBench represents an effort in benchmarking disinformation detection and opens venues for collaboration from researchers and practitioners to address this challenge.</p>
          <br>
          <p>For additional details about VLDBench evaluation and experimental results, please refer to our main paper.
            Thank you! </p>
        </div>

        <div class="columns is-centered">
          <div class="column is-full-width has-text-centered">
            <h2 class="title is-3">Social Statement</h2>
          </div>
        </div>
        <div class="content has-text-justified">
          <p> Disinformation threatens democratic institutions, public trust, and social cohesion. <b>Generative AI
              exacerbates the problem</b> by enabling sophisticated multimodal campaigns that exploit cultural,
            political, and linguistic nuances, requiring solutions beyond technical approaches. </p>
          <p> <b>VLDBench</b> addresses this challenge as the first multimodal benchmark for disinformation detection,
            combining text and image analysis with ethical safeguards. It prioritizes <b>cultural sensitivity</b>
            through regional annotations and mitigates bias with audits and human-AI hybrid validation. Ground-truth
            labels are sourced from fact-checked references with transparent provenance tracking. </p>
          <p> As both a <b>technical resource</b> and a <b>catalyst for collaboration</b>, VLDBench democratizes
            access to cutting-edge detection tools by open-sourcing its benchmark and models. It highlights systemic
            risks, like adversarial attack vulnerabilities, to drive <b>safer and more reliable systems</b>. Designed
            to foster partnerships across academia, industry, journalism, and policymaking, VLDBench bridges the gap
            between research and real-world impact. </p>
          <p> <b>Ethical risks</b> are carefully addressed through restricted access, exclusion of synthetic tools,
            and human oversight requirements. Representation gaps in non-English content are documented to guide
            future adaptations. Binding agreements prohibit harmful applications such as censorship, surveillance, or
            targeted disinformation campaigns. </p>
          <p> By focusing exclusively on disinformation detection, VLDBench supports <b>media literacy</b>, unbiased
            fact-checking, and policy discussions on AI governance. Its <b>ethical design</b> and <b>equitable
              access</b> empower communities and institutions to combat disinformation while fostering trust in
            digital ecosystems. </p>
        </div>
      </div>
    </div>



      

  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title"><a id="bibtex">BibTeX</a></h2>
      <pre><code>To be released!
</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p class="has-text-centered">
            Website adapted from the following <a href="https://mingukkang.github.io/GigaGAN/">source code</a>.
          </p>
        </div>
      </div>
    </div>
    </div>
  </footer>


  <script src="juxtapose/js/juxtapose.js"></script>

  <script>
    var slider;
    let origOptions = {
      "makeResponsive": true,
      "showLabels": true,
      "mode": "horizontal",
      "showCredits": true,
      "animate": true,
      "startingPosition": "50"
    };

    const juxtaposeSelector = "#juxtapose-embed";
    const transientSelector = "#juxtapose-hidden";

    inputImage.src = "./static/images/".concat(name, "_input.jpg")
    outputImage.src = "./static/images/".concat(name, "_output.jpg")

    let images = [inputImage, outputImage];
    let options = slider.options;
    options.callback = function (obj) {
      var newNode = document.getElementById(obj.selector.substring(1));
      var oldNode = document.getElementById(juxtaposeSelector.substring(1));
      console.log(obj.selector.substring(1));
      console.log(newNode.children[0]);
      oldNode.replaceChild(newNode.children[0], oldNode.children[0]);
      //newNode.removeChild(newNode.children[0]);

    };

    slider = new juxtapose.JXSlider(transientSelector, images, options);
};



    (function () {
      slider = new juxtapose.JXSlider(
        juxtaposeSelector, origImages, origOptions);
      //document.getElementById("left-button").onclick = replaceLeft;
      //document.getElementById("right-button").onclick = replaceRight;
    })();
    // Get the image text
    var imgText = document.getElementById("imgtext");
    // Use the same src in the expanded image as the image being clicked on from the grid
    // expandImg.src = imgs.src;
    // Use the value of the alt attribute of the clickable image as text inside the expanded image
    imgText.innerHTML = name;
    // Show the container element (hidden with CSS)
    // expandImg.parentElement.style.display = "block";

    $(".flip-card").click(function () {
      console.log("fading in")
      div_back = $(this).children().children()[1]
      div_front = $(this).children().children()[0]
      // console.log($(this).children("div.flip-card-back"))
      console.log(div_back)
      $(div_front).addClass("out");
      $(div_front).removeClass("in");

      $(div_back).addClass("in");
      $(div_back).removeClass("out");

    });

    $(".flip-card").mouseleave(function () {
      console.log("fading in")
      div_back = $(this).children().children()[1]
      div_front = $(this).children().children()[0]
      // console.log($(this).children("div.flip-card-back"))
      console.log(div_back)
      $(div_front).addClass("in");
      $(div_front).removeClass("out");

      $(div_back).addClass("out");
      $(div_back).removeClass("in");

    });

  </script>
  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script> -->
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js"
    integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>

</body>

</html>