<!DOCTYPE html>
<html lang="en">

<head>
  <title>VLDBench</title>
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script src="https://kit.fontawesome.com/f8ddf9854a.js" crossorigin="anonymous"></script>
  <meta charset="utf-8">
  <meta name="description"
    content="A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI">
  <meta name="keywords"
    content="MMMU, LMM, LMM Evaluation, Vision Language Model, Large Language Model, Large Multimodal Model, artificial intelligence, AI, AGI, artificial general intelligence">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title> VLDBench: Vision Language Models Disinformation Detection Benchmark</title>

  <link rel="icon" href="./static/images/mmmu_icon2.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <script src="https://kit.fontawesome.com/fff5b27ec1.js" crossorigin="anonymous"></script>
  <!-- <script src="https://kit.fontawesome.com/eaf1856e6f.js" crossorigin="anonymous"></script> -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
      <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
          <div class="navbar-item has-dropdown is-hoverable">
            <a class="navbar-link">
              More Research
            </a>
            <div class="navbar-dropdown">
              <a class="navbar-item" href="https://huggingface.co/datasets/MMMU/MMMU_Pro">
                <b>MMMU-Pro</b> <span style="font-size:18px; display: inline; margin-left: 5px;">🔥</span>
              </a>
              <a class="navbar-item" href="https://tiger-ai-lab.github.io/MAmmoTH/">
                MAmmoTH
              </a>
              <a class="navbar-item" href="https://osu-nlp-group.github.io/TableLlama/">
                TableLlama
              </a>
              <a class="navbar-item" href="https://osu-nlp-group.github.io/MagicBrush/">
                MagicBrush
              </a>
              <a class="navbar-item" href="https://osu-nlp-group.github.io/Mind2Web/">
                Mind2Web
              </a>
            </div>
          </div>
        </div>
      </div>
    </nav> -->

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title is-bold">
              <img src="static/images/mmmu_icon2.png" style="width:1em;vertical-align: middle" alt="Logo" />
              <span class="mmmu" style="vertical-align: middle">VLDBench</span>
            </h1>
            <h2 class="subtitle is-3 publication-subtitle">
              <u>V</u>ision <u>L</u>anguage Models <u>D</u>isinformation Detection <u>Bench</u>mark
            </h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://shainaraza.github.io/" style="text-decoration: none; color: inherit;">Shaina
                  Raza*†</a>,
              </span>
              <span class="author-block">
                <a href="https://ashmalvayani.github.io/" style="text-decoration: none; color: inherit;">Ashmal
                  Vayani*</a>,
              </span>
              <span class="author-block">
                <a href="https://vectorinstitute.ai/team/elham-dolatabadi-2/"
                  style="text-decoration: none; color: inherit;">Elham Dolatabadi*</a>,
              </span>
              <span class="author-block">
                <a href="https://aravind-3105.github.io/" style="text-decoration: none; color: inherit;">Aravind
                  Narayanan*</a>,
              </span>
              <span class="author-block">
                <a href="https://web.cse.ohio-state.edu/~sun.397/"
                  style="text-decoration: none; color: inherit;">Aditya Jain*</a>,
              </span>
              <span class="author-block">
                <a href="https://ysu1989.github.io/" style="text-decoration: none; color: inherit;">Vahid Reza Khazaie*†</a>,
              </span>
              <span class="author-block">
                <a href="https://wenhuchen.github.io/" style="text-decoration: none; color: inherit;">Syed Raza Bashir*†</a>
              </span>
              <span class="author-block">
                <a href="https://wenhuchen.github.io/" style="text-decoration: none; color: inherit;">Gias Uddin*†</a>
              </span>
              <span class="author-block">
                <a href="https://wenhuchen.github.io/" style="text-decoration: none; color: inherit;">Rizwan Qureshi*†</a>
              </span>
              <span class="author-block">
                <a href="https://wenhuchen.github.io/" style="text-decoration: none; color: inherit;">Christos Emmanouilidis*†</a>
              </span>
              <span class="author-block">
                <a href="https://wenhuchen.github.io/" style="text-decoration: none; color: inherit;">Mubarak Shah*†</a>
              </span>
            </div>

            <br>

            <div class="is-size-5 publication-authors">
              <!-- <span class="author-block"><b>MMMU Team</b></span> -->
            </div>

            <br>
            <div class="is-size-5 publication-authors">
              <span class="author-block">*Core Contributors</span><br>
              <span class="author-block">†Corresponding to:</span>
              <!-- <span class="author-block"><a href="mailto:xiangyue.work@gmail.com">xiangyue.work@gmail.com</a>,</span> -->
              <!-- <span class="author-block"><a href="mailto:su.809@osu.edu">su.809@osu.edu</a>,</span> -->
              <!-- <span class="author-block"><a href="mailto:wenhuchen@uwaterloo.ca">wenhuchen@uwaterloo.ca</a></span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2311.16502" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- <span class="link-block">
                  <a href="https://huggingface.co/datasets/vector-institute/newsmediabias-plus-clean"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon" style="font-size:18px">🤗</span>
                    <span>NMB+</span>
                  </a>
                </span> -->
                <!-- <span class="link-block">
                    <a href="https://huggingface.co/datasets/MMMU/MMMU" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="font-size:18px">🤗</span>
                      <span>MMMU</span>
                    </a>
                  </span> -->
                <span class="link-block">
                  <a href="https://github.com/MMMU-Benchmark/MMMU"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop has-text-centered">
      <img src="static/images/architecture.png" alt="VLDBench Overview" , class="center" style="width: 100%;" />
      <p>VLDBench is a multimodal disinformation detection benchmark that operates through a threestage pipeline: (1)
        <b><i>Data Curation</i></b> (collect/filter text-image pairs), (2) <b><i>Annotation</i></b> (GPT-4o labeling +
        human validation), (3) <b><i>Benchmarking</i></b> (model evaluation) </p>

    </div>
  </section>

  <section class="section">
    <div class="container" style="margin-bottom: 2vh;">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Introduction</h2>
          <div class="content has-text-justified">
            <p>
              The rapid growth of generative AI (GenAI) has heightened the challenge of detecting multimodal
              disinformation—deliberately false information designed to deceive—on digital platforms. While existing AI
              safety benchmarks primarily address bias and toxicity, disinformation detection remains largely
              underexplored. To bridge this gap, we introduce the Vision-Language Disinformation Detection Benchmark
              (<b>VLDBench</b>), a comprehensive benchmark to date for identifying disinformation in both unimodal
              (text-only) and multimodal (text and image) content. <b>VLDBench</b> features a rigorous semi-automated
              data curation and annotation pipeline, covering diverse news categories. We extensively evaluate
              state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs), demonstrating that
              integrating textual and visual cues in multimodal news posts improves disinformation detection accuracy by
              5–15% compared to unimodal models. Developed in alignment with AI governance frameworks such as the EU AI
              Act, NIST guidelines, and the MIT AI Risk Repository 2024, <b>VLDBench</b> serves as a critical resource
              for testing models and applications aimed at detecting and mitigating disinformation in online content.
              Our code and data will be made publicly available.
            </p>
            <!-- Add image -->
            <div class="content has-text-centered">
              <img src="static/images/example_page-0001.jpg" alt="Example" , class="center" style="width: 50%;" />
              <p>Examples of visual disinformation and
                how misleading contexts reinforce false narratives.</p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <!-- DATASET SECTION -->
  <section class="hero is-light is-small">
    <div class="hero-body has-text-centered">
      <h1 class="title is-1 mmmu">
        <img src="static/images/mmmu_icon2.png" alt="Logo" class="mmmu-logo" />
        <span class="mmmu">VLD Benchmark</span>
      </h1>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Overview</h2>
          <div class="content has-text-justified">
            <p>
              VLDBench, is a comprehensive classification multimodal benchmark for disinformation detection in news
              articles. It includes over 31k disinformation images curated from 43 news sources ranging from the
              Financial Times, CNN, and New York Times to Axios and Wall Street Journal. VLDBench spans 12 unique
              categories: National, Business and Finance, International, Entertainment, Local/Regional,
              Opinion/Editorial, Health, Sports, Politics, Weather and Environment, Technology, Science, and
              Other—adding depth to the disinformation domains.
            </p>
            <!-- <img src="static/images/mmlu_example.Jpeg" alt="algebraic reasoning" class="center"> -->
            <!-- <br> -->
          </div>
          <h2 class="title is-3">Key contributions</h2>
          <div class="content has-text-justified">
            <p>
              <b>Lowering Barriers to Disinformation Research:</b>
              VLDBench addresses the resource-intensive nature of disinformation research by providing a meticulously
              curated multimodal dataset. It includes guidelines for training models in resource-constrained settings,
              such as academic labs, ensuring accessibility and scalability. By open-sourcing the dataset and code,
              VLDBench democratizes research and fosters collaboration in the disinformation detection domain.
            </p>
            <p>
              <b>Annotating for Subjectivity and Consistency:</b> Recognizing the contextual and adversarial challenges
              of disinformation labeling, VLDBench employs a multi-annotator strategy using aggregated GPT-4o
              annotations. This approach ensures reduced randomness and balanced representation, maintaining consistency
              across ambiguous or contested cases.
            </p>
            <p><b>Rigorous Human-AI Validation:</b>
              VLDBench integrates human and AI validation to ensure robustness. (1) Domain experts reviewed AI-generated
              annotations, achieving a high inter-annotator agreement (Cohen’s κ = 0.82). (2) Independent human
              evaluations confirmed the AI-assisted annotations with an F1 score of 0.92, demonstrating the
              complementary role of AI in refining human judgments.</p>
          </div>

          <h2 class="title is-3">Annotation Pipeline</h2>
          <div class="content has-text-justified">
            <p>
              VLDBench represents the <b>largest and most comprehensive human-verified disinformation detection benchmark</b>, with over <b>300 hours of human verification</b>. Using a semi-automated pipeline, each article was categorized by GPT-4o as either "Likely" or "Unlikely" to contain disinformation. This binary categorization ensures a balance between nuanced evaluation and manageability.
            </p></p>
              To enhance reliability, GPT-4o evaluated text-image alignment three times per sample to minimize variance and resolve potential ties in classification. Its selection was based on demonstrated effectiveness in both textual and visual reasoning tasks. The annotations were systematically reviewed by 22 domain experts, who assessed accuracy, consistency, and alignment with human judgment. This rigorous process resulted in strong inter-annotator agreement with a Cohen’s κ of 0.82. </p><p>
              The figure below provides an example of disinformation narratives analyzed by GPT-4o, illustrating confidence levels and reasoning.
            </p>
            <!-- Add image -->
             <div class="content has-text-centered">
              <img src="static/images/qualitative_page-0001.jpg" alt="Annotation Pipeline" width="100%">
              <p> Analysis of disinformation narratives and confidence levels from GPT-4o.</p>
        </div>

      

      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content">
          <h2 class="title is-3">Dataset Statistics</h2>
          <div class="carousel results-carousel">
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/outlet_distribution.jpg" alt="algebraic reasoning" width="95%" />
                <p> Distribution of Articles Across Various News Outlets. This bar chart illustrates the number of
                  articles published by each outlet in our dataset, highlighting the predominant sources of news
                  coverage.</p>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/news_categories_distribution_page-0001.jpg" alt="dataset statistics"
                  width="75%" />
                <p>Category distribution with overlaps. Total unique articles = 31,339. Percentages sum to > 100% due to
                  multi-category articles.</p>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/key_dataset_statistics.png" alt="dataset statistics" width="75%" />
                <p> Key dataset statistics</p>
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/sentiment_distributions.png" alt="algebraic reasoning" width="95%" />
                <p> Sentiment polarity and subjectivity distributions for the dataset. The left plot shows polarity
                  (positive, neutral, and negative sentiments), while the right plot displays subjectivity ranging from
                  objective to subjective content.</p>
              </div>
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content">
          <h2 class="title is-3">Perturbation Generation</h2>
          <div class="content has-text-justified">
            <table border="1" style="width:100%; border-collapse:collapse; text-align:left;">
              <!-- <caption style="font-weight:bold; text-align:center; margin-bottom:10px;">
                Table 12: Description of Perturbation Types
              </caption> -->
              <thead>
                <tr>
                  <th>Perturbation</th>
                  <th>Description</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><strong>Text Perturbations (T-P)</strong></td>
                  <td>Adversarial modifications applied to textual inputs, such as word substitutions, paraphrasing, or negation-based changes. These perturbations test the model’s robustness to textual manipulations.</td>
                </tr>
                <tr>
                  <td><strong>Image Perturbations (I-P)</strong></td>
                  <td>Visual modifications applied to images, including noise addition, blurring, or adversarial transformations. These perturbations assess the model’s sensitivity to altered visual inputs.</td>
                </tr>
                <tr>
                  <td><strong>Cross-Modal Misalignment/Match (C-M)</strong></td>
                  <td>Disruptions in the alignment between textual and visual inputs. Examples include mismatched image captions, misleading textual descriptions, or contradictory multimodal content.</td>
                </tr>
                <tr>
                  <td><strong>Both-Modality Perturbations (B-P)</strong></td>
                  <td>Combined perturbations where both text and image distortions are applied simultaneously. This simulates real-world misinformation scenarios where misleading text and visuals coexist.</td>
                </tr>
              </tbody>
            </table>            
            <p> <strong>Textual Perturbations:</strong> These include various manipulations of text to test model robustness: </p>
            <ul>
              <li><strong>Synonym Substitution:</strong> Using the TextAttack library, sentences undergo word substitutions while
                preserving grammatical correctness.</li>
              <li><strong>Misspellings:</strong> Character-level errors are introduced by swapping or inserting characters, with a
                20% probability of perturbation per word.</li>
              <li><strong>Negation:</strong> Words like "not" or "never" are inserted after auxiliary or modal verbs, often altering
                sentiment or stance detection.</li>
            </ul>
            <p> <strong>Example:</strong> </p>
            <ul>
              <li><em><b>Original:</b></em> "I love the pictures you posted from last night’s party."</li>
              <li><em><b>ynonym Substitution:</b></em> "I adore the photos you shared from yesterday’s gathering."</li>
              <li><em><b>Misspelling:</b></em> "I love the pictures you poested from lat night’s pary."</li>
              <li><em><b>Negation:</b></em> "I do not love the pictures you posted from last night’s party."</li>
            </ul>
            <p> <strong>Image Perturbations:</strong> These include modifications to visual inputs: </p>
            <ul>
              <li><strong>Blurring:</strong> Applying Gaussian blur with a kernel size of (3,3).</li>
              <li><strong>Noise:</strong> Adding Gaussian noise with mean 0 and standard deviation 0.1 to pixel values.</li>
              <li><strong>Resizing:</strong> Scaling images to 50% or 200% of their original sizes.</li>
            </ul>
            <p> <strong>Cross-Modal Perturbations:</strong> These involve disruptions between text and visual inputs: </p>
            <ul>
              <li><strong>Mismatched Image-Text Pairs:</strong> Swapping images and captions from unrelated contexts to create
                misalignment.</li>
              <li><strong>Contradictory Captions:</strong> Modifying captions to conflict with the image content (e.g., labeling a
                protest image as a "peaceful gathering").</li>
            </ul>
        </div>
      </div>

    </div>


    



    <div class="container">
      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content">
          <h2 class="title is-3">LLMs and VLMs used for Evaluation</h2>
          <div class="content has-text-justified">
            <table>
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Model Name</th>
                </tr>
              </thead>
              <tbody> <!-- Language-Only LLMs -->
                <tr>
                  <td rowspan="6">Language-Only LLMs</td>
                  <td>Phi-3-mini-128k-instruct</td>
                </tr>
                <tr>
                  <td>Vicuna-7b-v1.5</td>
                </tr>
                <tr>
                  <td>Mistral-7B-Instruct-v0.3</td>
                </tr>
                <tr>
                  <td>Qwen2-7B-Instruct</td>
                </tr>
                <tr>
                  <td>Llama-3.1-8B-Instruct</td>
                </tr>
                <tr>
                  <td>Llama-3.2-1B-Instruct</td>
                </tr> <!-- Vision-Language Models (VLMs) -->
                <tr>
                  <td rowspan="10">Vision-Language Models (VLMs)</td>
                  <td>Phi-3-vision-128k-instruct</td>
                </tr>
                <tr>
                  <td>Deepseek Janus-Pro-7B</td>
                </tr>
                <tr>
                  <td>Deepseek-vl2-small</td>
                </tr>
                <tr>
                  <td>Llava-v1.5-vicuna7b</td>
                </tr>
                <tr>
                  <td>Llava-v1.6-mistral-7b</td>
                </tr>
                <tr>
                  <td>Pixtral</td>
                </tr>
                <tr>
                  <td>Qwen2-VL-7B-Instruct</td>
                </tr>
                <tr>
                  <td>Llama-3.2-11B-Vision</td>
                </tr>
                <tr>
                  <td>GLM-4V-9B</td>
                </tr>
                <tr>
                  <td>InternVL2-8B</td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>
      </div>




  </section>




  <!-- RESULTS SECTION -->
  <section class="hero is-light is-small">
    <div class="hero-body has-text-centered">
      <h1 class="title is-1 mmmu">Experiment Results</h1>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content">
          <h2 class="title is-3" id="leaderboard">Overview</h2>
          <div class="content has-text-justified">
            <p>
              Our investigation focuses on three core ques404 tions: <b>(1)</b> Does multimodal (text+image) data
              improve disinformation detection compared to text alone? <b>(2)</b> Does instruction-based fine tuning
              enhance generalization and robustness? <b>(3)</b> How vulnerable are models to adversarial perturbations
              in text and images?
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="container">
      <!-------------------------------------------------------------------- RESULTS SECTION -------------------------------------------------------------------->
      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content">
          <h2 class="title is-3" id="leaderboard">Comparison of Unimodality and Multimodality.</h2>
          <div class="content has-text-justified">
            <div class="content has-text-centered">
              <img src="static/images/zero-shot_performance.png" alt="Zero Shot Perofrmance" width="75%">
              <p> Zero-shot performance of language-only models (top) vs. vision-language models (bottom) on
                our dataset. <b>Bold</b> = best in each metric.</p>
            </div>
            <p>
              The above table shows that VLMs consistently outperform unimodal LLMs. For example, Llava-v1.6-Mistral-7B
              achieves an F1 of 69.89% vs. 66.56% from its unimodal counterpart, Mistral-7B. Similarly, <b>Pixtral
                obtains the highest F1 (70.70%)</b>, clearly exceeding that of language-only models. Across different
              model families (e.g., Phi-3 vs. Phi-3-vision, Qwen2 vs. Qwen2-VL), the <b>multimodal variants yield
                stronger detection rates</b>. These results reinforce the benefit of incorporating both textual and
              visual cues for disinformation detection
            </p>
          </div>

          <h2 class="title is-3" id="leaderboard">Instruction Fine-Tuning Enhances Performance</h2>
            <div class="content has-text-centered">
              <img src="static/images/ift.png" alt="Zero Shot Robustness" width="75%">
              <p> Comparison of zero-shot vs. instruction-fine-tuned (IFT) performance, with 95% confidence intervals computed from three independent runs.
              </p>
            </div>
            <p align="justify">
              Instruction fine-tuning (IFT) on models like Phi, Mistral-Llava, Qwen, and Llama3.2—along with their vision-language counterparts—using the VLDBench training subset led to significant performance improvements. For instance, Phi-3-Vision-IFT achieved a 7% improvement over its base model, with similar gains observed in the text-only Phi-3-mini-128k model. These improvements highlight the models’ ability to adapt to disinformation-specific cues from VLDBench, showcasing the dataset's quality and effectiveness.
            </p>


          <h2 class="title is-3" id="leaderboard">Robustness to Adversarial Perturbations</h2>
          <h3 class="title is-4">Text and Image Attacks</h3>

          <p align="justify">
            We evaluated models under controlled adversarial conditions, such as textual synonyms, misspellings, and
            image modifications (blurring/resizing). Results showed that Vision-Language Models (VLMs) handle
            single-modality attacks effectively but struggle with cross-modal mismatches and combined perturbations.
            Language-Only Models (LLMs) showed higher vulnerability to text changes, with significant performance drops
            (e.g., LLama3.2-1B saw a 9.44% accuracy loss under text negations). Large VLMs experienced severe
            degradation, with combined text-image attacks reducing accuracy by up to 26.4%. These findings highlight the
            need for improved robustness in multimodal systems.
          </p>
          <div class="content has-text-justified">
            <div class="content has-text-centered">
              <img src="static/images/zero-shot_robustness.png" alt="Zero Shot Robustness" width="50%">
              <p> Zero-shot robustness to perturbations: TP (text attacks), I-P (image attacks), C-M (crossmodal
                mismatch), B-P (combined). Red shading indicates larger F1 drop.
              </p>
            </div>
          </div>


          <h3 class="title is-4">Combined Attacks</h3>

          <p align="justify">
            The below table reveals that combining text and image adversarial attacks can result in drastic performance
            drops, particularly in high-capacity models like LLama3.2-11B, which experienced a 26.4% accuracy decrease.
            While multimodal models achieve higher baseline accuracy, they remain vulnerable to coordinated attacks
            targeting both modalities. These findings underscore the importance of developing more robust architectures
            and leveraging data augmentation techniques to mitigate the effects of such adversarial challenges in future
            iterations.
          </p>
          <div class="content has-text-justified">
            <div class="content has-text-centered">
              <img src="static/images/Adversarial-Peformance.png" alt="Advarsarial Performance" width="50%">
              <p> Adversarial Performance Drops (∆ =
                Accuracy Decrease). Combined text+image attacks
              </p>
            </div>
          </div>

          <h3 class="title is-4">Transformation Examples</h3>
          <p align="justify">
            The table illustrates how progressive visual degradations (e.g., blur, resizing, added noise) can mislead
            the model, culminating in incorrect classification of previously detected disinformation.
          </p>
          <div class="content has-text-justified">
            <div class="content has-text-centered">
              <img src="static/images/VisualTransformations.png" alt="Advarsarial Performance" width="80%">
              <p> Visual transformations and their impact on disinformation classification. Noise and downscaling
                can shift model outputs from “Yes” to “No.”
              </p>
            </div>
          </div>



          <h2 class="title is-3" id="leaderboard">Human Evaluation Validates Accuracy and Reasoning</h2>

          <p align="justify">
            To assess model performance, a balanced 500-sample test set (250 disinformation, 250 neutral) was used,
            where three VLMs classified samples and provided reasoning. Human evaluators, blind to model identities,
            rated outputs based on <b>Prediction Correctness (PC)</b> and <b>Reasoning Clarity (RC)</b> on a 1–5 scale.
            Llama3.2-11B achieved the highest accuracy (75.2%) and most coherent explanations. Tables 5 and 7 will
            highlight these evaluations and emphasize differences in model outputs, confirming VLDBench’s efficacy in
            disinformation detection testing.
          </p>

          <div class="content has-text-justified">
            <div class="content has-text-centered">
              <img src="static/images/HumanEvaluation.png" alt="Human Evaluation" width="50%">
              <p> Human evaluation results on a 500-sample
                test set. PC = prediction correctness, RC = reasoning clarity. Mean ± std. shown </p>
            </div>
            <div class="content has-text-centered">
              <img src="static/images/reasoning_page-0001.jpg" alt="Human Evaluation" width="60%">
              <p> Human evaluation results on a 500-
                sample test set. Models were tasked with classifying disinformation and justifying their predictions. PC
                = prediction correctness, RC = reasoning clarity (mean ± std.). </p>
            </div>
          </div>
          <!-------------------------------------------------------------------- Error Example  -------------------------------------------------------------------->
          <div class="columns is-centered m-6">
            <div class="column is-full has-text-centered content">
              <h2 class="title is-3" id="examples">Prompts</h2>
              <div class="carousel results-carousel">
                <div class="box m-5">
                  <div class="content has-text-centered">
                    <img src="static/images/Prompt1.png" alt="grade-lv" width="70%"/>
                    <p>Disinformation Prompt for Texts</p>
                  </div>
                </div>
                <div class="box m-5">
                  <div class="content has-text-centered">
                    <img src="static/images/Prompt2.png" alt="grade-lv" width="70%"/>
                    <p>Disinformation Prompt for Multimodal Data</p>
                  </div>
                </div>
                <div class="box m-5">
                  <div class="content has-text-centered">
                    <img src="static/images/Prompt3.png" alt="grade-lv" width="70%"/>
                    <p>Evaluator’s Prompt for Texts Disinformation Assessment</p>
                  </div>
                </div>
                <div class="box m-5">
                  <div class="content has-text-centered">
                    <img src="static/images/Prompt4.png" alt="grade-lv" width="70%"/>
                    <p>Evaluator’s Prompt for Disinformation Assessment (Text and Image)</p>
                  </div>
                </div>
            </div>
          </div>
        </div>
        </div>
      </div>

      <section class="hero is-light is-small">
        <div class="hero-body has-text-centered">
          <h1 class="title is-1 mmmu">Social Statement</h1>
        </div>
      </section>

      <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content">
          <!-- <h2 class="title is-3">Error Analysis</h2> -->
          <div class="content has-text-justified">
            <p> Disinformation threatens democratic institutions, public trust, and social cohesion. <b>Generative AI
                exacerbates the problem</b> by enabling sophisticated multimodal campaigns that exploit cultural,
              political, and linguistic nuances, requiring solutions beyond technical approaches. </p>
            <p> <b>VLDBench</b> addresses this challenge as the first multimodal benchmark for disinformation detection,
              combining text and image analysis with ethical safeguards. It prioritizes <b>cultural sensitivity</b>
              through regional annotations and mitigates bias with audits and human-AI hybrid validation. Ground-truth
              labels are sourced from fact-checked references with transparent provenance tracking. </p>
            <p> As both a <b>technical resource</b> and a <b>catalyst for collaboration</b>, VLDBench democratizes
              access to cutting-edge detection tools by open-sourcing its benchmark and models. It highlights systemic
              risks, like adversarial attack vulnerabilities, to drive <b>safer and more reliable systems</b>. Designed
              to foster partnerships across academia, industry, journalism, and policymaking, VLDBench bridges the gap
              between research and real-world impact. </p>
            <p> <b>Ethical risks</b> are carefully addressed through restricted access, exclusion of synthetic tools,
              and human oversight requirements. Representation gaps in non-English content are documented to guide
              future adaptations. Binding agreements prohibit harmful applications such as censorship, surveillance, or
              targeted disinformation campaigns. </p>
            <p> By focusing exclusively on disinformation detection, VLDBench supports <b>media literacy</b>, unbiased
              fact-checking, and policy discussions on AI governance. Its <b>ethical design</b> and <b>equitable
                access</b> empower communities and institutions to combat disinformation while fostering trust in
              digital ecosystems. </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- @PAN TODO: bibtex -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title is-3 has-text-centered">BibTeX</h2>
      <pre><code>
          @inproceedings{yue2023mmmu,
            title={MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI},
            author={Xiang Yue and Yuansheng Ni and Kai Zhang and Tianyu Zheng and Ruoqi Liu and Ge Zhang and Samuel Stevens and Dongfu Jiang and Weiming Ren and Yuxuan Sun and Cong Wei and Botao Yu and Ruibin Yuan and Renliang Sun and Ming Yin and Boyuan Zheng and Zhenzhu Yang and Yibo Liu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
            booktitle={Proceedings of CVPR},
            year={2024},
          }
    </code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a>, <a
              href="https://mathvista.github.io/">MathVista</a> and <a href="https://mmmu-benchmark.github.io/">MMMU</a>
            licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>